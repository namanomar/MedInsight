# MedInsight 

> AI-powered medical knowledge assistant built on Retrieval-Augmented Generation (RAG).

## ğŸ—ï¸ Project Structure

```
MedInsight/
â”œâ”€â”€ llm/                    # LLM module
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ ollama_client.py    # Ollama integration for Qwen 3 4B
â”‚
â”œâ”€â”€ memory/                 # Vector store module
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ vector_store.py     # FAISS vector store operations
â”‚
â”œâ”€â”€ prompts/                # Prompt templates
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ rag_prompt.py       # RAG prompts with few-shot examples
â”‚
â”œâ”€â”€ utils/                  # Utility functions
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ helpers.py          # Helper functions
â”‚
â”œâ”€â”€ eval/                   # Evaluation module
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ rag_evaluator.py    # RAG system evaluation
â”‚
â”œâ”€â”€ config.py               # Centralized configuration
â”œâ”€â”€ main.py                 # Streamlit web application
â”œâ”€â”€ connect_llm_with_memory.py  # CLI chatbot
â”œâ”€â”€ llm_memory.py          # Data ingestion pipeline
â””â”€â”€ requirements.txt        # Python dependencies
```

## ğŸš€ Quick Start

### 1. Install Dependencies

```bash
pip install -r requirements.txt
```

### 2. Setup Ollama

Install Ollama from https://ollama.ai and pull the Qwen 3 4B model:

```bash
ollama pull qwen2.5:4b
```

Ensure Ollama is running:

```bash
ollama serve
```

### 3. Ingest Documents

Place your PDF files in the `data/` directory and run:

```bash
python llm_memory.py
```

### 4. Run the Application

**Web UI:**

```bash
streamlit run main.py
```

**CLI:**

```bash
python connect_llm_with_memory.py
```

## ğŸ“‹ Configuration

Edit `config.py` or set environment variables:

- `OLLAMA_BASE_URL`: Ollama API URL (default: http://localhost:11434)
- `LLM_MODEL_NAME`: Model name (default: qwen2.5:4b)
- `LLM_TEMPERATURE`: Sampling temperature (default: 0.5)
- `LLM_MAX_NEW_TOKENS`: Max tokens to generate (default: 512)
- `EMBEDDING_MODEL`: Embedding model (default: sentence-transformers/all-MiniLM-L6-v2)
- `RETRIEVER_TOP_K`: Number of documents to retrieve (default: 3)
- `USE_PROMPT_EXAMPLES`: Enable few-shot examples (default: true)

## ğŸ”§ Module Details

### LLM Module (`llm/`)

- **Ollama Integration**: Connects to Ollama API
- **Model**: Qwen 3 4B (qwen2.5:4b)
- **Connection Checking**: Validates Ollama availability

### Memory Module (`memory/`)

- **Vector Store**: FAISS-based document storage
- **Embeddings**: HuggingFace sentence transformers
- **Retrieval**: Top-K similarity search

### Prompts Module (`prompts/`)

- **Templates**: Detailed RAG prompt templates
- **Few-shot Examples**: Includes examples for better understanding
- **Validation**: Input validation for prompts

### Utils Module (`utils/`)

- **Source Formatting**: Formats source citations
- **Connection Checking**: Ollama connection validation
- **Logging**: Setup logging configuration

### Eval Module (`eval/`)

- **Evaluation Metrics**: Relevance scoring, answer quality assessment
- **Reports**: Generate evaluation reports

## ğŸ¯ Features

- âœ… Modern modular architecture
- âœ… Ollama integration with Qwen 3 4B
- âœ… Detailed prompt templates with examples
- âœ… Input validation and error handling
- âœ… Evaluation framework
- âœ… Both web UI and CLI interfaces

## ğŸ“ Notes

- Ensure Ollama is running before starting the application
- The model name should match what you've pulled in Ollama (e.g., `qwen2.5:4b`)
- Prompt examples can be disabled by setting `USE_PROMPT_EXAMPLES=false`
